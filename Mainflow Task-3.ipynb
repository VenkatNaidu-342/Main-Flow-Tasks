{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14b1ab4-d131-459b-aa72-ccc09260fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Web Scrape Digital Journal News Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "Blog\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download\n",
      "Pricing\n",
      "Free Courses\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              parsehub.com\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "27 February 2023\n",
      "\n",
      "How to Web Scrape Digital Journal News Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this blog, we will teach you how to scrape news headlines, authors and content from ¬†DigitalJournal.com, and any other news outlet. To get started, download our free web scraper, ParseHub!Digital Journal has been covering news since 1998 and has over 70,000 articles published every single month. It was founded in Mississauga, Ontario in Canada. In 2001, Digital Journal began publishing magazines which it distributed all over Canada. Now they work with hundreds of large-scale publishers and PR firms. Being a big repository of the latest news and trends, there are a number of news categories you can scrape from, such as international news, tech, science, social media, business, entertainment, lifestyle, sports and much more.Let‚Äôs start scraping news articles!Step 1: Scraping HeadlinesBegin by opening the ParseHub application and clicking the blue ‚ÄúNew Project‚Äù button.Enter the Digital Journal URL you wish to scrape from, we will be the latest tech articles with this URL: https://www.digitaljournal.com/tech-scienceOnce the page loads on ParseHub‚Äôs browser, click the first article‚Äôs headline to extract it.The rest of the articles will turn yellow, click the next one to train the algorithm.Rename this selection on the left to ‚Äúarticle‚Äù.Step 2: Scraping Article DetailsClick the PLUS(+) button next to your ‚Äúarticle‚Äù selection, and choose ‚ÄúClick‚Äù.Choose ‚ÄúNo‚Äù on the popup, as this is not a next page button, we are going to each article to extract its contents.Create a new template with the green ‚ÄúCreate New Template‚Äù button.Wait for the article page to load, then click its image, to extract it.Rename this selection on the left to ‚Äúimage‚Äù.Click the PLUS(+) button next to the ‚Äúpage‚Äù selection and choose ‚ÄúSelect‚Äù.Hover over the article‚Äôs text content, click and hold CTR/CMD+1 until the full div is selected, then click your mouse to extract it.Rename this extraction to ‚Äúcontent‚Äù on the left.Step 3: Begin Digital Journal ScrapingNow that you have extracted headlines, and have set up a ParseHub template to extract each article‚Äôs inner data, you are ready to begin scraping!Click the green ‚ÄúGet Data‚Äù button on the left to begin. You can choose to Test, Run or Schedule your scrape. We chose ‚ÄúRun‚Äù to scrape a single time on ParseHub‚Äôs server. Your scrape results should look similar to this:P.S. You can also scrape HTML data from each article, instead of just text.Need help scraping articles, trends or other websites? Contact our live chat support.Happy Scraping! üíª\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Farzad Vafaei\n",
      "Read more posts by this author.\n",
      "\n",
      "\n",
      "\n",
      "Read More\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Scrape Samsung Products\n",
      "\n",
      "\n",
      "In this tutorial, we will show you how to scrape any product on Samsung‚Äôs website for free, including mobile phones, laptops, monitors and more with ParseHub!\n",
      "\n",
      "Samsung is a South Korean company,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Farzad Vafaei\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 min read\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Scrape Flipkart Products\n",
      "\n",
      "\n",
      "In this tutorial, you will learn how to scrape large amounts of Flipkart product data with our free web scraper, ParseHub.\n",
      "\n",
      "Flipkart was founded in 2007 by former Amazon employees, and is India‚Äô\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Farzad Vafaei\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 min read\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Web Scraping Blog (Tips, Guides + Tutorials) | ParseHub\n",
      "\n",
      "\n",
      "‚Äî\n",
      "How to Web Scrape Digital Journal News Articles\n",
      "\n",
      "Share this \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Web Scraping Blog (Tips, Guides + Tutorials) | ParseHub ¬© 2024\n",
      "\n",
      "Latest Posts\n",
      "Facebook\n",
      "Twitter\n",
      "Ghost\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Links:\n",
      "/blog\n",
      "/blog\n",
      "https://www.parsehub.com/quickstart\n",
      "https://www.parsehub.com/pricing\n",
      "https://academy.parsehub.com/\n",
      "https://www.facebook.com/ParseHubApp/\n",
      "https://twitter.com/parsehub\n",
      "https://feedly.com/i/subscription/feed/https://www.parsehub.com/blog/rss/\n",
      "/\n",
      "https://parsehub.com/?ref=parsehub.com\n",
      "https://www.digitaljournal.com/tech-science?ref=parsehub.com\n",
      "https://www.parsehub.com/blog/html-scraping/\n",
      "https://parsehub.com/?ref=parsehub.com\n",
      "/blog/author/farzad/\n",
      "/blog/author/farzad/\n",
      "/blog/author/farzad/\n",
      "/blog/scrape-samsung-products/\n",
      "/blog/scrape-samsung-products/\n",
      "/blog/author/farzad/\n",
      "/blog/scrape-flipkart-products/\n",
      "/blog/scrape-flipkart-products/\n",
      "/blog/author/farzad/\n",
      "https://www.parsehub.com/blog\n",
      "https://twitter.com/share?text=How%20to%20Web%20Scrape%20Digital%20Journal%20News%20Articles&url=https://www.parsehub.com/blog/scrape-digital-journal-news-articles/\n",
      "https://www.facebook.com/sharer/sharer.php?u=https://www.parsehub.com/blog/scrape-digital-journal-news-articles/\n",
      "https://www.parsehub.com/blog\n",
      "https://www.parsehub.com/blog\n",
      "https://www.facebook.com/ParseHubApp/\n",
      "https://twitter.com/parsehub\n",
      "https://ghost.org\n",
      "\n",
      "Images:\n",
      "/blog/assets/images/parsehub_logo3.svg?v=22c6426566\n",
      "/blog/assets/images/parsehub_logo_icon.svg?v=22c6426566\n",
      "/blog/assets/images/exit.svg?v=22c6426566\n",
      "/blog/content/images/size/w2000/2023/02/How-to-Scrape-Digital-Journal--1-.jpg\n",
      "https://lh5.googleusercontent.com/d0W3fB1FozEJ5UMVv8Rd-r8-30xXU9ljJq_x0bPJxCH84YtiQlDn5K3H_J9HRxvwUdnEfFGIf-wSfAa3LejYeYpmb1sy089TuANkc1FwT2-8Zf9d1rC0otlCCDIjTTxOpIfSFTdECA31dfjyGqQ9sAc\n",
      "https://lh3.googleusercontent.com/yE3lAhEgD4dHjD3EEOUg1FIGN4dTXaZnZoQ5HI30LwJp-BWmCpxbBf_ycl_lxGPYXMQGGvSnyOr3inPpYABJ9o_eVMX8t3sk44XzgEjL5yA0UeynlzeGs2-oVDSDwsQoDwgi2OBhpIzifLTXABSgJpo\n",
      "https://lh4.googleusercontent.com/5yK84IRrcXTBSh61btJiJgvh4Ht7naZtNumqu7WBKcxblrRjPoVNv4zWH5OQ_9-ymcMrEnQ8Qrm65OldlkIrLIQbDb9D5Z_uGUvRoX8DCD1TyT1XN2ngdPXgn5jC6IG-WuF7l_ay9Qp9oGX66ZWbDoI\n",
      "/blog/content/images/size/w100/2022/06/profilepic.jpg\n",
      "/blog/content/images/size/w600/2023/03/How-to-Scrape-Samsung--1-.jpg\n",
      "/blog/content/images/size/w100/2022/06/profilepic.jpg\n",
      "/blog/content/images/size/w600/2023/02/How-to-Scrape-Flipkart--1-.jpg\n",
      "/blog/content/images/size/w100/2022/06/profilepic.jpg\n",
      "/blog/content/images/size/w30/2019/08/parsehub_logo2.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the web page to scrape\n",
    "url = 'https://parsehub.com/blog/scrape-digital-journal-news-articles/' # Replace with the URL of the web page you want to scrape\n",
    "\n",
    "# Send a GET request to the web page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract all the text from the page\n",
    "    page_text = soup.get_text()\n",
    "\n",
    "    # Extract all the links from the page\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "    # Extract all the images from the page\n",
    "    images = [img['src'] for img in soup.find_all('img', src=True)]\n",
    "\n",
    "    # Print the extracted data\n",
    "    print(\"Page text:\")\n",
    "    print(page_text)\n",
    "\n",
    "    print('\\nLinks:')\n",
    "    for link in links:\n",
    "        print(link)\n",
    "    \n",
    "    print('\\nImages:')\n",
    "    for image in images:\n",
    "        print(image)\n",
    "\n",
    "else:\n",
    "    print(f'Failed to retrieve the web page. Status Code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba86e26-497f-45e3-bb2c-70954cdbcd74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
